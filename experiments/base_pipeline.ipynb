{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/classification_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first one need to split data to train and test and then launch client with vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent_id': 0,\n",
       " 'intent_name': 'activate_my_card',\n",
       " 'sample_utterances': [\"Please help me with my card.  It won't activate.\",\n",
       "  'I tired but an unable to activate my card.',\n",
       "  'I want to start using my card.',\n",
       "  'How do I verify my new card?',\n",
       "  \"I tried activating my plug-in and it didn't piece of work\"],\n",
       " 'regexp_for_sampling': [],\n",
       " 'regexp_as_rules': []}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "banking77 = json.load(open('../data/records/banking77.json'))\n",
    "banking77[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_sample_utterances(dataset: list[dict]):\n",
    "    \"\"\"get plain list of all sample utterances and their intent labels\"\"\"\n",
    "    utterances = [intent['sample_utterances'] for intent in dataset]\n",
    "    labels = [[intent['intent_id']] * len(uts) for intent, uts in zip(dataset, utterances)]\n",
    "\n",
    "    utterances = list(it.chain.from_iterable(utterances))\n",
    "    labels = list(it.chain.from_iterable(labels))\n",
    "\n",
    "    return utterances, labels\n",
    "\n",
    "\n",
    "def split_sample_utterances(dataset: list[dict]):\n",
    "    \"\"\"\n",
    "    Return: utterances_train, utterances_test, labels_train, labels_test\n",
    "    \n",
    "    TODO: ensure stratified train test splitting (test set must contain all classes)\n",
    "    \"\"\"\n",
    "\n",
    "    utterances, labels = get_sample_utterances(dataset)\n",
    "\n",
    "    return train_test_split(\n",
    "        utterances,\n",
    "        labels,\n",
    "        test_size=0.25,\n",
    "        random_state=0,\n",
    "        stratify=labels,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 97)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances_train, utterances_test, labels_train, labels_test = split_sample_utterances(banking77)\n",
    "len(utterances_train), len(utterances_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction as EmbFunc\n",
    "from chromadb import ClientAPI\n",
    "\n",
    "\n",
    "def create_collection(\n",
    "    utterances: list[str],\n",
    "    labels: list[int],\n",
    "    client: ClientAPI,\n",
    "    name=\"example_collection\",\n",
    "    embedder_name=\"Alibaba-NLP/gte-base-en-v1.5\",\n",
    "):\n",
    "    labels_set = set(labels)\n",
    "    n_classes = len(labels_set)\n",
    "    assert set(range(n_classes)) == labels_set, \"labels must be from [0,n_classes-1]\"\n",
    "\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=name,\n",
    "        embedding_function=EmbFunc(model_name=embedder_name, trust_remote_code=True),\n",
    "        metadata={'n_classes': n_classes}\n",
    "    )\n",
    "\n",
    "    collection.add(\n",
    "        documents=utterances,\n",
    "        ids=[str(i) for i in range(len(utterances))],\n",
    "        metadatas=[{'intent_id': lab} for lab in labels]\n",
    "    )\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voorhs/.pyenv/versions/3.10.14/envs/.autointent-dev/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "\n",
    "\n",
    "client = PersistentClient(path='../data/chroma')\n",
    "client.delete_collection(\"example_collection\")\n",
    "\n",
    "collection = create_collection(\n",
    "    utterances_train,\n",
    "    labels_train,\n",
    "    client\n",
    ")\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction as EmbFunc\n",
    "\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, intent_records: os.PathLike, db_path: os.PathLike = '../data/chroma'):\n",
    "        self.utterances_train, self.utterances_test, self.labels_train, self.labels_test = split_sample_utterances(intent_records)\n",
    "\n",
    "        self.client = PersistentClient(path=db_path)\n",
    "    \n",
    "    def create_collection(self, model_name: str, db_name: str = \"example_collection\"):\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=db_name,\n",
    "            embedding_function=EmbFunc(model_name=model_name, trust_remote_code=True),\n",
    "            metadata={'n_classes': len(set(self.labels_train))}\n",
    "        )\n",
    "        collection.add(\n",
    "            documents=self.utterances_train,\n",
    "            ids=[str(i) for i in range(len(self.utterances_train))],\n",
    "            metadatas=[{'intent_id': lab} for lab in self.labels_train]\n",
    "        )\n",
    "        self.collection = collection\n",
    "        return collection\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class Module:\n",
    "    def fit(self, data_handler: DataHandler):\n",
    "        pass\n",
    "\n",
    "    def score(self, data_handler: DataHandler, metric_fn: Callable) -> float:\n",
    "        pass\n",
    "\n",
    "    def fit_score(self, data_handler: DataHandler, metric_fn: Callable) -> float:\n",
    "        self.fit(data_handler)\n",
    "        return self.score(data_handler, metric_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent_id': 0,\n",
       " 'intent_name': 'what_are_you_talking_about',\n",
       " 'sample_utterances': [],\n",
       " 'regexp_for_sampling': ['(alexa ){0,1}what are ((you)|(we)) ((talking about)|(discussing))',\n",
       "  '(alexa ){0,1}what ((you)|(we)) are (even ){0,1}((talking about)|(discussing))',\n",
       "  '(alexa ){0,1}what does it mean',\n",
       "  '(alexa ){0,1}pass that by me again',\n",
       "  \"(alexa ){0,1}i ((don't)|(didn't)|(do not)|(did not)) get it\",\n",
       "  '(alexa ){0,1}what it is about',\n",
       "  '(alexa ){0,1}what is it about',\n",
       "  'i lost common ground',\n",
       "  '(alexa ){0,1}what (even ){0,1}is that',\n",
       "  \"(i ((did not get)|(don't understand)|(don't get)) ){0,1}what do you mean( alexa){0,1}\",\n",
       "  \"(sorry, ){0,1}i ((don't)|(do not)|(didn't)|(did not)) ((understand)|(get))( ((what you mean)|(what are you talking about)))( alexa){0,1}\",\n",
       "  '((what you mean)|(what are you talking about))( alexa){0,1}',\n",
       "  \"i don't know what you just said\"],\n",
       " 'regexp_as_rules': ['(alexa ){0,1}are we having a communication problem',\n",
       "  \"(alexa ){0,1}i don't think you understand\",\n",
       "  'what',\n",
       "  'I did not get what do you mean']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "dream = json.load(open('../data/records/dream.json'))\n",
    "dream[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def regexp(utterance: str, intents_patterns: list[dict]):\n",
    "    detected = set()\n",
    "    for intent in intents_patterns:\n",
    "        for pattern in intent['regexp_for_sampling'] + intent['regexp_as_rules']:\n",
    "            if re.match(pattern, utterance) is None:\n",
    "                continue\n",
    "            detected.add(intent['intent_id'])\n",
    "    return detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp(\n",
    "    utterance='what are you talking about',\n",
    "    intents_patterns=dream\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp(\n",
    "    utterance='tell me something else',\n",
    "    intents_patterns=dream\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp(\n",
    "    utterance='kind of',\n",
    "    intents_patterns=dream\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Collection\n",
    "\n",
    "\n",
    "def retrieval(utterance: str, collection: Collection, k: int):\n",
    "    query_res = collection.query(\n",
    "        query_texts=[utterance],\n",
    "        n_results=k,\n",
    "        include=[\"metadatas\", \"documents\"]  # one can add \"embeddings\", \"distances\"\n",
    "    )\n",
    "    return query_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = 'i want a new card'\n",
    "query_res = retrieval(utterance, collection, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['40', '23', '240']],\n",
       " 'distances': None,\n",
       " 'metadatas': [[{'intent_id': 39}, {'intent_id': 39}, {'intent_id': 43}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['I want some extra physical cards.',\n",
       "   \"I'd like to order an additional card\",\n",
       "   'Can I request a card?']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class VectorDBModule(Module):\n",
    "    def __init__(self, model_name: str, k: int):\n",
    "        self.model_name = model_name\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, data_handler: DataHandler):\n",
    "        data_handler.create_collection(self.model_name)\n",
    "\n",
    "    def score(self, data_handler: DataHandler, metric_fn: Callable):\n",
    "        query_res = data_handler.collection.query(\n",
    "            query_texts=data_handler.utterances_test,\n",
    "            n_results=self.k,\n",
    "            include=[\"metadatas\", \"documents\"]  # one can add \"embeddings\", \"distances\"\n",
    "        )\n",
    "        labels_pred = [[cand['intent_id'] for cand in candidates] for candidates in query_res['metadatas']]\n",
    "        return metric_fn(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modules:\n",
    "- knn\n",
    "- linear\n",
    "- dnnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Literal\n",
    "\n",
    "\n",
    "# class ScorerOptimizer:\n",
    "#     available_modules = {\n",
    "#         'knn': KNNScorer,\n",
    "#         'linear': LinearScorer,\n",
    "#         'dnnc': DNNCScorer,\n",
    "#     }\n",
    "#     def __init__(self, module_type: Literal['knn', 'linear', 'dnnc'], **hyperparams):\n",
    "#         self.module = self.available_modules[module_type](**hyperparams)\n",
    "    \n",
    "#     def fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ScoringModule(Module):\n",
    "    def score(self, data_handler: DataHandler, metric_fn: Callable):\n",
    "        probas = self.predict(data_handler.utterances_test)\n",
    "        return metric_fn(data_handler.labels_test, probas)\n",
    "\n",
    "    def predict(self, utterances: list[str]):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict_topk(self, utterance: str, k=3):\n",
    "        scores = self.predict(utterance)\n",
    "        top_indices = np.argpartition(scores, kth=-k)[-k:]\n",
    "        top_scores = scores[top_indices]\n",
    "        return top_indices[np.argsort(top_scores)][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNNScorer(ScoringModule):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    - add weighted knn?\n",
    "    \"\"\"\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, data_handler: DataHandler):\n",
    "        self._collection = data_handler.collection\n",
    "        self._n_classes = data_handler.collection.metadata['n_classes']\n",
    "\n",
    "    def predict(self, utterances: list[str]):\n",
    "        \"\"\"\n",
    "        TODO: test this code\n",
    "        \"\"\"\n",
    "        query_res = self._collection.query(\n",
    "            query_texts=utterances,\n",
    "            n_results=self.k,\n",
    "            include=[\"metadatas\", \"documents\"]  # one can add \"embeddings\", \"distances\"\n",
    "        )\n",
    "        labels_pred = [[cand['intent_id'] for cand in candidates] for candidates in query_res['metadatas']]\n",
    "        y = np.array(labels_pred)\n",
    "\n",
    "        n_queries = len(utterances)\n",
    "        n_classes = self._collection.metadata['n_classes']\n",
    "        y += n_classes * np.arange(n_queries)[:, None]\n",
    "        counts = np.bincount(y.ravel(), minlength=n_classes*n_queries).reshape(n_queries, n_classes)\n",
    "        \n",
    "        return counts / counts.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.1, 0. , 0. , 0.1,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.1, 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0.2, 0.2, 0.1, 0. , 0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_scorer = KNNScorer(k=10)\n",
    "knn_scorer.fit(collection)\n",
    "knn_scorer.predict_proba('i want a new card')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40, 39, 12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_scorer.predict_topk('i want a new card')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "\n",
    "class LinearScorer(ScoringModule):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    - implement different modes (incremental learning with SGD and simple learning with LogisticRegression)\n",
    "    - control n_jobs\n",
    "    - adjust cv\n",
    "    - ensure that embeddings of train set are not recalculated\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, data_handler: DataHandler):\n",
    "        dataset = data_handler.collection.get(include=['embeddings', 'metadatas'])\n",
    "        features = dataset['embeddings']\n",
    "        labels = [dct['intent_id'] for dct in dataset['metadatas']]\n",
    "        clf = LogisticRegressionCV(cv=3, n_jobs=8, multi_class='multinomial')\n",
    "        clf.fit(features, labels)\n",
    "\n",
    "        self._clf = clf\n",
    "        self._emb_func = data_handler.collection._embedding_function\n",
    "    \n",
    "    def predict(self, utterances: list[str]):\n",
    "        features = self._emb_func(utterances)\n",
    "        return self._clf.predict_proba(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voorhs/.pyenv/versions/3.10.14/envs/.autointent-dev/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1905: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "linear_scorer = LinearScorer()\n",
    "linear_scorer.fit(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.36315606e-02, 2.06736865e-03, 1.07819996e-03, 2.84117638e-03,\n",
       "       1.06543725e-03, 4.14270486e-04, 4.56166199e-04, 9.31523609e-04,\n",
       "       8.16927696e-04, 1.04347418e-01, 4.14846962e-03, 1.07265188e-02,\n",
       "       2.87261476e-02, 2.22940571e-02, 4.22492414e-02, 2.29241231e-03,\n",
       "       3.62684145e-03, 1.13575060e-03, 3.57048793e-03, 5.06323550e-04,\n",
       "       1.13312843e-03, 9.95915608e-03, 7.87386713e-04, 1.55948122e-02,\n",
       "       1.01617718e-02, 1.36223739e-02, 7.43484887e-04, 1.21027345e-03,\n",
       "       1.65229683e-03, 1.14075433e-02, 2.32802378e-02, 1.24439988e-03,\n",
       "       8.63093249e-04, 3.74815527e-03, 1.91142796e-03, 5.09926445e-04,\n",
       "       8.36122168e-04, 4.88069047e-03, 2.72668840e-03, 2.16004925e-01,\n",
       "       5.72615173e-02, 1.98442906e-01, 7.95156098e-04, 5.20176928e-02,\n",
       "       1.22214771e-03, 1.21927601e-03, 6.29880435e-04, 6.33465044e-03,\n",
       "       3.60845091e-04, 1.24068092e-03, 8.97284111e-04, 1.52737685e-03,\n",
       "       3.98661705e-03, 2.60228011e-03, 3.17184426e-03, 1.78225807e-03,\n",
       "       2.85753976e-04, 3.84157723e-03, 5.25502069e-03, 1.07554061e-03,\n",
       "       1.72601972e-03, 2.13645632e-04, 9.34382564e-03, 6.65626960e-03,\n",
       "       3.35275014e-04, 6.05367711e-04, 3.82808060e-04, 7.57821268e-04,\n",
       "       2.03855134e-03, 4.67869006e-04, 6.38749651e-04, 3.33442563e-03,\n",
       "       2.01741784e-03, 1.48651561e-02, 2.16785278e-03, 8.85086591e-04,\n",
       "       4.09359403e-04])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_scorer.predict_proba('i want a new card')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39, 41,  9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_scorer.predict_topk('i want a new card')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99926525, 0.99926525], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model_name = \"BAAI/bge-reranker-base\"\n",
    "cross_encoder = CrossEncoder(model_name, trust_remote_code=True)\n",
    "cross_encoder.predict([['i want a new card', 'new card please'], ['i want a new card', 'new card please']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Collection\n",
    "\n",
    "\n",
    "class DNNCScorer(ScoringModule):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    - think about other cross-encoder settings\n",
    "    - implement training of cross-encoder with sentence_encoders utils\n",
    "    - control device of model\n",
    "    - inspect batch size of model.predict?\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, k: int):\n",
    "        self.model = CrossEncoder(model_name, trust_remote_code=True)\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, data_handler: DataHandler):\n",
    "        self._collection = data_handler.collection\n",
    "    \n",
    "    def predict(self, utterances: list[str]):\n",
    "        \"\"\"\n",
    "        returns just a smooth indicator of the chosen class\n",
    "        \n",
    "        TODO: test this code\n",
    "        \"\"\"\n",
    "        query_res = self._collection.query(\n",
    "            query_texts=utterances,\n",
    "            n_results=self.k,\n",
    "            include=[\"metadatas\", \"documents\"]  # one can add \"embeddings\", \"distances\"\n",
    "        )\n",
    "\n",
    "        text_pairs = [[[query, cand] for cand in q_res] for query, q_res in zip(utterances, query_res['documents'])]\n",
    "        flattened_text_pairs = list(it.chain.from_iterable(text_pairs))\n",
    "        flattened_cross_encoder_scores = self.model.predict(flattened_text_pairs)\n",
    "        cross_encoder_scores = [flattened_cross_encoder_scores[i:i+self.k] for i in range(0,len(flattened_cross_encoder_scores,self.k))]\n",
    "\n",
    "        labels_pred = [[cand['intent_id'] for cand in candidates] for candidates in query_res['metadatas']]\n",
    "        res = np.zeros((len(utterances), self._collection.metadata['n_classes']))\n",
    "        i_best = np.argmax(cross_encoder_scores, axis=1)\n",
    "        res[labels_pred[i_best]] = cross_encoder_scores[i_best]\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnc_scorer = DNNCScorer(model_name=\"BAAI/bge-reranker-base\", k=10)\n",
    "dnnc_scorer.fit(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.99974364, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnnc_scorer.predict_proba('i want a card')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnnc_scorer.predict_topk('i want a card', k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision(scores: list[float], thresholds: list[float] | float):\n",
    "    \"\"\"\n",
    "    Arguments: classwise `scores` and classwise `thresholds` (or a single one)\n",
    "    \"\"\"\n",
    "    i_best = np.argmax(scores)\n",
    "    score_best = scores[i_best]\n",
    "\n",
    "    thresh = thresholds[i_best] if isinstance(thresholds, list) else thresholds\n",
    "\n",
    "    if score_best > thresh:\n",
    "        return i_best\n",
    "\n",
    "    return None # i.e. out of scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModule(Module):\n",
    "    def __init__(self, single_thresh: bool):\n",
    "        self.signle_thresh = single_thresh\n",
    "\n",
    "    def fit(self, data_handler: DataHandler):\n",
    "        n_classes = data_handler.collection.metadata['n_classes']\n",
    "        self.thresh = 0.5 if self.signle_thresh else np.ones(n_classes) / 2\n",
    "        \n",
    "        # TODO: optimization\n",
    "\n",
    "    def score(self, data_handler: DataHandler, metric_fn: Callable):\n",
    "        predictions = self.predict(data_handler.scores)\n",
    "        return metric_fn(data_handler.labels_test, predictions)\n",
    "\n",
    "    def predict(self, scores: list[list[float]]):\n",
    "        pred_classes = np.argmax(scores, axis=1)\n",
    "        thresh = self.thresh if self.signle_thresh else self.thresh[pred_classes]\n",
    "        best_scores = scores[pred_classes]\n",
    "        pred_classes[best_scores < thresh] = None\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score(query_labels: list[int], candidates_labels: list[list[int]]):\n",
    "#     \"\"\"\n",
    "#     Arguments\n",
    "#     ---\n",
    "#     - `query_labels`: for each query, this list contains its class labels\n",
    "#     - `candidates_labels`: for each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)\n",
    "#     - `k`: the number of top items to consider for each query\n",
    "\n",
    "#     Return\n",
    "#     ---\n",
    "#     retrieval metric, averaged over all queries\n",
    "    \n",
    "\n",
    "#     TODO:\n",
    "#     - implement multilabel case, where query_labels: list[list[int]], i.e. each query has multiple intents\n",
    "#     \"\"\"\n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5833333333333333, 0.5833333333333333)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def average_precision(query_label: int, candidate_labels: list[int], k: int = None) -> float:\n",
    "    num_relevant = 0\n",
    "    sum_precision = 0.0\n",
    "    for i, label in enumerate(candidate_labels[:k]):\n",
    "        if label == query_label:\n",
    "            num_relevant += 1\n",
    "            sum_precision += num_relevant / (i + 1)\n",
    "    return sum_precision / num_relevant if num_relevant > 0 else 0.0\n",
    "\n",
    "def retrieval_map(query_labels: list[int], candidates_labels: list[list[int]], k: int = None):\n",
    "    ap_list = [average_precision(q, c, k) for q, c in zip(query_labels, candidates_labels)]\n",
    "    return sum(ap_list) / len(ap_list)\n",
    "\n",
    "def retrieval_map_numpy(query_labels: list[int], candidates_labels: list[list[int]], k: int) -> float:\n",
    "    query_labels = np.array(query_labels)\n",
    "    candidates_labels = np.array(candidates_labels)\n",
    "    candidates_labels = candidates_labels[:, :k]\n",
    "    relevance_mask = (candidates_labels == query_labels[:, None])\n",
    "    cumulative_relevant = np.cumsum(relevance_mask, axis=1)\n",
    "    precision_at_k = cumulative_relevant * relevance_mask / np.arange(1, k + 1)\n",
    "    sum_precision = np.sum(precision_at_k, axis=1)\n",
    "    num_relevant = np.sum(relevance_mask, axis=1)\n",
    "    average_precision = np.divide(sum_precision, num_relevant, out=np.zeros_like(sum_precision), where=num_relevant != 0)\n",
    "    return np.mean(average_precision)\n",
    "\n",
    "y_true = 1\n",
    "y_pred = [2,1,1]\n",
    "retrieval_map([y_true], [y_pred], k=3), retrieval_map_numpy([y_true], [y_pred], k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieval_hit_rate(query_labels: list[int], candidates_labels: list[list[int]], k: int) -> float:\n",
    "    num_queries = len(query_labels)\n",
    "    hit_count = 0\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        query_label = query_labels[i]\n",
    "        candidate_labels = candidates_labels[i][:k]\n",
    "\n",
    "        if query_label in candidate_labels:\n",
    "            hit_count += 1\n",
    "\n",
    "    return hit_count / num_queries\n",
    "\n",
    "def retrieval_hit_rate_numpy(query_labels: list[int], candidates_labels: list[list[int]], k: int) -> float:\n",
    "    query_labels = np.array(query_labels)\n",
    "    candidates_labels = np.array(candidates_labels)\n",
    "    truncated_candidates = candidates_labels[:, :k]\n",
    "    hit_mask = np.isin(query_labels[:, None], truncated_candidates).any(axis=1)\n",
    "    hit_rate = hit_mask.mean()\n",
    "    return hit_rate\n",
    "\n",
    "query_labels = [1]\n",
    "candidates_labels = [[1, 4, 5, 2]]\n",
    "k = 2\n",
    "\n",
    "retrieval_hit_rate(query_labels, candidates_labels, k), retrieval_hit_rate_numpy(query_labels, candidates_labels, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.6666666666666666)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieval_precision(query_labels: list[int], candidates_labels: list[list[int]], k: int) -> float:\n",
    "    total_precision = 0.0\n",
    "    num_queries = len(query_labels)\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        query_label = query_labels[i]\n",
    "        candidate_labels = candidates_labels[i][:k]\n",
    "\n",
    "        relevant_items = [label for label in candidate_labels if label == query_label]\n",
    "        precision_at_k = len(relevant_items) / k\n",
    "\n",
    "        total_precision += precision_at_k\n",
    "\n",
    "    return total_precision / num_queries\n",
    "\n",
    "\n",
    "def retrieval_precision_numpy(query_labels: list[int], candidates_labels: list[list[int]], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---\n",
    "    - `query_labels`: for each query, this list contains its class labels\n",
    "    - `candidates_labels`: for each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)\n",
    "    - `k`: the number of top items to consider for each query\n",
    "\n",
    "    Return\n",
    "    ---\n",
    "    retrieval metric, averaged over all queries\n",
    "    \"\"\"\n",
    "    query_labels = np.array(query_labels)\n",
    "    candidates_labels = np.array(candidates_labels)\n",
    "    top_k_candidates = candidates_labels[:, :k]\n",
    "    matches = (top_k_candidates == query_labels[:, None]).astype(int)\n",
    "    relevant_counts = np.sum(matches, axis=1)\n",
    "    precision_at_k = relevant_counts / k\n",
    "    return np.mean(precision_at_k)\n",
    "\n",
    "\n",
    "query_labels = [1]\n",
    "candidates_labels = [[1, 1, 3, 4, 5]]\n",
    "k = 3\n",
    "\n",
    "retrieval_precision(query_labels, candidates_labels, k), retrieval_precision_numpy(query_labels, candidates_labels, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9197207891481876"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dcg(relevance_scores, k):\n",
    "    \"\"\"\n",
    "    Calculate the Discounted Cumulative Gain (DCG) at position k.\n",
    "\n",
    "    Arguments\n",
    "    ---\n",
    "    - `relevance_scores`: numpy array of relevance scores for items\n",
    "    - `k`: the number of top items to consider\n",
    "\n",
    "    Return\n",
    "    ---\n",
    "    DCG value at position k\n",
    "    \"\"\"\n",
    "    relevance_scores = relevance_scores[:k]\n",
    "    discounts = np.log2(np.arange(2, k + 2))\n",
    "    dcg = np.sum((2 ** relevance_scores - 1) / discounts)\n",
    "    return dcg\n",
    "\n",
    "def idcg(relevance_scores, k):\n",
    "    \"\"\"\n",
    "    Calculate the Ideal Discounted Cumulative Gain (IDCG) at position k.\n",
    "\n",
    "    Arguments\n",
    "    ---\n",
    "    - `relevance_scores`: numpy array of relevance scores for items\n",
    "    - `k`: the number of top items to consider\n",
    "\n",
    "    Return\n",
    "    ---\n",
    "    IDCG value at position k\n",
    "    \"\"\"\n",
    "    ideal_scores = np.sort(relevance_scores)[::-1]\n",
    "    return dcg(ideal_scores, k)\n",
    "\n",
    "\n",
    "def retrieval_ndcg(query_labels, candidates_labels, k):\n",
    "    ndcg_scores = []\n",
    "    relevance_scores = np.array(query_labels)[:, None] == np.array(candidates_labels)\n",
    "\n",
    "    for rel_scores in relevance_scores:\n",
    "        cur_dcg = dcg(rel_scores, k)\n",
    "        cur_idcg = idcg(rel_scores, k)\n",
    "        ndcg_scores.append(0.0 if cur_idcg == 0 else cur_dcg / cur_idcg)\n",
    "\n",
    "    return sum(ndcg_scores) / len(ndcg_scores)\n",
    "\n",
    "query_labels = [1]\n",
    "candidates_labels = [[1, 2, 1, 2, 5]]\n",
    "k = 3\n",
    "\n",
    "retrieval_ndcg(query_labels, candidates_labels, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_mrr(query_labels: list[int], candidates_labels: list[list[int]]) -> float:\n",
    "    mrr_sum = 0.0\n",
    "    num_queries = len(query_labels)\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        query_label = query_labels[i]\n",
    "        candidate_labels = candidates_labels[i]\n",
    "\n",
    "        for rank, label in enumerate(candidate_labels):\n",
    "            if label == query_label:\n",
    "                mrr_sum += 1.0 / (rank + 1)\n",
    "                break\n",
    "\n",
    "    mrr = mrr_sum / num_queries\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalNode:\n",
    "    metrics_available = {\n",
    "        'retrieval_map': retrieval_map,\n",
    "        'retrieval_ndcg': retrieval_ndcg,\n",
    "        'retrieval_hit_rate': retrieval_hit_rate,\n",
    "        'retrieval_precision': retrieval_precision,\n",
    "        'retrieval_mrr': retrieval_mrr\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model_names: list[str],\n",
    "        k: int,\n",
    "        metric: Literal['retrieval_map', 'retrieval_ndcg', 'retrieval_hit_rate', 'retrieval_precision', 'retrieval_mrr']\n",
    "    ):\n",
    "        self.embedding_model_names = embedding_model_names\n",
    "        self.client = PersistentClient(path='../data/chroma')\n",
    "        self.k = k\n",
    "        self.metric_name = metric\n",
    "        self.metric_fn = self.metrics_available[metric]\n",
    "\n",
    "    def fit(self, dataset: list[dict]):\n",
    "        \"\"\"\n",
    "        `dataset`: intent records\n",
    "\n",
    "        TODO: add splits statistics to optimization results (train size, test size, how many instances of each class in each split)\n",
    "        \"\"\"\n",
    "        splits = split_sample_utterances(dataset)\n",
    "        metric_scores = [self._score_embedder(emb_name, *splits) for emb_name in self.embedding_model_names]\n",
    "        self.optimization_results = {\n",
    "            'metric_name': self.metric_name,\n",
    "            'i_best': np.argmax(metric_scores),\n",
    "            'scores': [{'model': model, 'score': score} for model, score in zip(self.embedding_model_names, metric_scores)]\n",
    "        }\n",
    "\n",
    "    def _score_embedder(self, embedder_name, utterances_train, utterances_test, labels_train, labels_test):\n",
    "        collection = create_collection(\n",
    "            utterances=utterances_train,\n",
    "            labels=labels_train,\n",
    "            client=self.client,\n",
    "            embedder_name=embedder_name\n",
    "        )\n",
    "        query_res = retrieval(utterances_test, collection, self.k)\n",
    "        labels_pred = [[cand['intent_id'] for cand in candidates] for candidates in query_res['metadatas']]\n",
    "        return self.metric_fn(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    metrics_available = {}\n",
    "    modules_available = {}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modules_search_spaces: list[dict],\n",
    "        metric: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `modules_search_spaces`: list of records, where each record is a mapping: hyperparam_name -> list of values (search space) with extra field \"module_type\" with values from [\"knn\", \"linear\", \"dnnc\"]\n",
    "        \"\"\"\n",
    "        self.modules_search_spaces = modules_search_spaces\n",
    "        self.metric_name = metric\n",
    "\n",
    "    def fit(self, data_handler: DataHandler):\n",
    "        metric_scores = []\n",
    "        modules_configs = []\n",
    "        for search_space in self.modules_search_spaces:\n",
    "            module_type = search_space.pop('module_type')\n",
    "            for module_config in it.product(*search_space.values()):\n",
    "                modules_configs.append(module_config)\n",
    "                module = self.modules_available[module_type](**module_config)\n",
    "                metric = module.fit_score(data_handler, self.metrics_available[self.metric_name])\n",
    "                metric_scores.append(metric)\n",
    "\n",
    "        self.optimization_results = {\n",
    "            'metric_name': self.metric_name,\n",
    "            'i_best': np.argmax(metric_scores),\n",
    "            'scores': metric_scores,\n",
    "            'configs': modules_configs\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorerNode(Node):\n",
    "    metrics_available = {\n",
    "        'neg_cross_entropy': None,\n",
    "        'roc_auc_ovr': None,\n",
    "        'roc_auc_ovo': None,\n",
    "    }\n",
    "\n",
    "    modules_available = {\n",
    "        'knn': KNNScorer,\n",
    "        'linear': LinearScorer,\n",
    "        'dnnc': DNNCScorer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalNode(Node):\n",
    "    metrics_available = {\n",
    "        'retrieval_map': retrieval_map,\n",
    "        'retrieval_ndcg': retrieval_ndcg,\n",
    "        'retrieval_hit_rate': retrieval_hit_rate,\n",
    "        'retrieval_precision': retrieval_precision,\n",
    "        'retrieval_mrr': retrieval_mrr\n",
    "    }\n",
    "\n",
    "    modules_available = {\n",
    "        'vector_db': VectorDBModule\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".autointent-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
